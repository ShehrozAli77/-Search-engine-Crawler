{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawler\n",
    "\n",
    "This notebook contains started code structure for creating a crawler on single machine\n",
    "\n",
    "**Author:** Noshaba Nasir\n",
    "\n",
    "**Date:** 9/4/2021\n",
    "\n",
    "**Updated by:** Shehroz Ali   L17-6334   IR_A\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "#### Each individual module of the crawler eg fetcher, Parser etc, are coded keep OOP in mind. Each module has it own class. Each module exposes specific apis that frontier can use to perform its task. These Seperate module instead of a whole one big frontier will allow me debug and add functionities with ease."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import requests as rq\n",
    "import mimetypes\n",
    "from urllib import parse, request, robotparser as botparser\n",
    "from queue import Queue\n",
    "import threading\n",
    "from queue import PriorityQueue\n",
    "from time import time, sleep\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BACKQUEUES = 3\n",
    "THREADS = BACKQUEUES * 3\n",
    "FRONTQUEUES = 5\n",
    "WAITTIME = 15  # wait 15 seconds before fetching URLS from\n",
    "urls_fetched = 0\n",
    "Filename = \"URLs.Json\" # File where all of the Raw Content will be dumped "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storage Class (Will handle content dumping to disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Class will handle everything related content storage\n",
    "class Storage:\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "        self.open = True  # Will let us know when if our file is open or closed\n",
    "        self.filehandle = open(self.filename, \"a\")\n",
    "\n",
    "    def PutContent(self, URL, Content):\n",
    "        json.dump({URL: Content}, self.filehandle)\n",
    "\n",
    "    def GetObjects(self):\n",
    "        return json.load(self.filehandle)\n",
    "\n",
    "    def __del__(self):\n",
    "        self.filehandle.close()\n",
    "        print(\"File Closed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetcher "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fetcher:  # will take a URL and Retrieve all the html URLS\n",
    "    opener = None\n",
    "    initialized = False\n",
    "\n",
    "    def __init__(self):\n",
    "        if not Fetcher.initialized:\n",
    "            Fetcher.opener = request.build_opener()\n",
    "            Fetcher.opener.addheaders = [('User-agent', 'Mozilla/5.0')]\n",
    "            request.install_opener(Fetcher.opener)\n",
    "            Fetcher.initialized = True\n",
    "    def fetch_Content(self, base_url):\n",
    "        Request_Url = rq.get(base_url)\n",
    "        soup = BeautifulSoup(Request_Url.text, \"html.parser\")\n",
    "        links = []\n",
    "\n",
    "        raw_content = Request_Url.text\n",
    "        x = soup.findAll('a', attrs={'href': re.compile(\"^http://|^https://|^/\")})\n",
    "\n",
    "        # for link in soup.findAll('a', attrs={'href': re.compile(\"^http://\")}):\n",
    "        for link in x:\n",
    "            # Convert relative link to absolute link if any\n",
    "            full_url = parse.urljoin(base_url, link.get('href'))\n",
    "            links.append(full_url)\n",
    "\n",
    "        return links, raw_content\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parser performs 2 Operations, It checks if the URL is duplicate by search in a URL list it Maintains, next it check if its allowed to fetch the page or not, this is done by reading the robot.txt file for the domain. The robot.txt object for a domain is keep in cache (list) and is reuse when a url of similar domain comes.  For assignment 1 Parser also perform a 3rd operation i.e is checking that the URL's \"content-type\" is text/html, It tried to determine the mime type using the URL itself. if its not sucessfull then and only then it request JUST the \"header\" NOT complete webpage to determine the mime type.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Parser:\n",
    "    # the parsor will maintain a list of URL that it haS seen (Duplicate Removal)\n",
    "    # the parser will also have a list of robot.txt for the domains it has seen\n",
    "    # Will also check if URl given is allowed to be fetched\n",
    "    def __init__(self):\n",
    "        self.URL_List = []  # if a URL is not in this it will added on first encounter\n",
    "        self.Robots_List = []\n",
    "\n",
    "    def Is_URL_Dup(self, URL):  # return true if URL is Duplicate and False if its not\n",
    "        if URL in self.URL_List:\n",
    "            return True\n",
    "        else:\n",
    "            # Add the URL in the List and Return false\n",
    "            self.URL_List.append(URL)\n",
    "            return False\n",
    "\n",
    "    def __Add_RobotTxt_Content__(self, URLDomain):\n",
    "        RP = botparser.RobotFileParser()\n",
    "        RP.set_url(\"http://\" + URLDomain + \"/robots.txt\")\n",
    "\n",
    "        try:\n",
    "            RP.read()\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "        # Add the RP Object to out RobotList_\n",
    "        self.Robots_List.append({URLDomain: RP})\n",
    "        # ret = next((i for i,item in enumerate(self.Robots_List) if URLDomain in item),None)\n",
    "        # return ret\n",
    "        return RP\n",
    "\n",
    "    def Is_URL_Allowed(self, URL):  # is URL Allowed to be fetched\n",
    "        URL_Domain = parse.urlparse(URL).netloc\n",
    "\n",
    "        P = next((d for i, d in enumerate(self.Robots_List) if URL_Domain in d), None)\n",
    "        if P == None:\n",
    "            P = self.__Add_RobotTxt_Content__(URL_Domain)\n",
    "            if P is None:\n",
    "                return True\n",
    "\n",
    "        # rp = P[URL_Domain]\n",
    "        try:\n",
    "            if P.can_fetch(\"*\", URL):\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        except Exception:\n",
    "            return True\n",
    "\n",
    "    def get_link_type(self, link, strict=True):\n",
    "        link_type, _ = mimetypes.guess_type(link)\n",
    "        if link_type is None and strict:\n",
    "            try:\n",
    "                u = rq.head(link)\n",
    "                # u = request.urlopen(link)\n",
    "                link_type = u.headers.get(\"content-type\", '')\n",
    "            except Exception:\n",
    "                link_type = \"\"\n",
    "        return link_type\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BackQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bqueue:  # back queue\n",
    "    MAX_URLS = 1000\n",
    "\n",
    "    def __init__(self):\n",
    "        self.queue = Queue(Bqueue.MAX_URLS)\n",
    "        self.domain = ''\n",
    "        self.empty = True\n",
    "\n",
    "    def get_domain(self):\n",
    "        return self.domain\n",
    "\n",
    "    def add_URL(self, URL):\n",
    "        if not self.queue.full():\n",
    "            # Check if queue is Empty to begin with\n",
    "            if self.empty:\n",
    "                self.empty = False\n",
    "                self.domain = parse.urlparse(URL).netloc\n",
    "\n",
    "            self.queue.put(URL)\n",
    "            return True\n",
    "        else:\n",
    "            # Queue is full\n",
    "            return False\n",
    "\n",
    "    def get_URL(self):\n",
    "        ret = self.queue.get()\n",
    "        if self.queue.empty():\n",
    "            self.empty = True\n",
    "        return ret\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FrontQueue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fqueue:  # front Queue\n",
    "    MAX_URLS = 1000\n",
    "\n",
    "    def __init__(self, Priority):\n",
    "        self.Priority = Priority\n",
    "        self.queue = Queue(Fqueue.MAX_URLS)\n",
    "\n",
    "    def get_Priority(self):\n",
    "        return self.Priority\n",
    "\n",
    "    def add_URL(self, URL):\n",
    "        if not self.queue.full():\n",
    "            self.queue.put(URL)\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def get_URL(self):\n",
    "        if not self.queue.empty():\n",
    "            return self.queue.get()\n",
    "        else:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FRONTIER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class frontier:\n",
    "    # add the code for frontier here\n",
    "    # should have functions __init__, get_URL, add_URLs, add_to_backqueue\n",
    "    def __init__(self, FrontQueues, Backqueues,\n",
    "                 filename):  # here FrontQueues and BackQueues are number of queues to be created\n",
    "        self.URL_Count = 0\n",
    "        # self.URL_List = []\n",
    "        self.FQueues = []  # list that will contain FQueue Objects\n",
    "        self.BQueues = []  # list that will contain Backqueue Objects\n",
    "        self.fetcher = Fetcher()\n",
    "        self.parser = Parser()\n",
    "        self.storage = Storage(filename)\n",
    "        self.TotalPriority = 0\n",
    "        self.TimeHeap = PriorityQueue()\n",
    "\n",
    "        for i in range(FrontQueues):\n",
    "            self.FQueues.append(Fqueue(i + 1))  # our Prority starts from 1\n",
    "\n",
    "        for i in range(Backqueues):\n",
    "            self.BQueues.append(Bqueue())\n",
    "\n",
    "        # Intiliaze the Heap Queue Also\n",
    "        for i in range(Backqueues):\n",
    "            self.TimeHeap.put((time(), i))  # Intilially time for threads will be zero at start\n",
    "            # The Tuple is in the following format (time,Index)\n",
    "\n",
    "    def add_urls_list(self, URLLIST):\n",
    "        # this function take a URLList and Perform Duplicate, Permission and mime type check and then add int to a\n",
    "        # Front Queue, return number of links added\n",
    "        i = 0\n",
    "        for url in URLLIST:\n",
    "            # perform a mime check first 3\n",
    "            if (self.parser.get_link_type(url)).find(\"text/html\") != -1:\n",
    "                # check if its Duplicate\n",
    "                if not self.parser.Is_URL_Dup(url):\n",
    "                    # Check if allowed to fetch\n",
    "                    if self.parser.Is_URL_Allowed(url):\n",
    "                        self.add_URL(url)\n",
    "                        i += 1\n",
    "                        # print(\"Count: \" +str(i) +\" Url Added to Front Queues: \" + url)\n",
    "                        print(\"Url Added to Front Queues: \" + url)\n",
    "                    else:\n",
    "                        print(\"url: \" + url + \" Rejected, Not allowed to access\")\n",
    "                else:\n",
    "                    print(\"url: \" + url + \" Rejected, Duplicate URL\")\n",
    "            else:\n",
    "                print(\"url: \" + url + \" Rejected, Invalid mime type (Not text/html)\")\n",
    "        return i\n",
    "\n",
    "    def add_URL(self, URL):\n",
    "        x = self.prioritizer()\n",
    "        self.FQueues[x].add_URL(URL)\n",
    "\n",
    "    def __Get_URL_From_Fqueues__(self):\n",
    "        return self.FQueues[self.prioritizer()].get_URL()  # This will also remove the URL from the queue\n",
    "\n",
    "    def __Are_All_FQueues_Empty(self):\n",
    "        for x in (self.FQueues):\n",
    "            if not x.queue.empty():\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def __has_domain__(self, URL):\n",
    "        # This function will return the queue if its a queue of it domain exists\n",
    "        for x in self.BQueues:\n",
    "            if not x.empty:\n",
    "                if x.domain == parse.urlparse(URL).netloc:\n",
    "                    return True, x\n",
    "\n",
    "        return False, None\n",
    "\n",
    "    def fill_Backqueue(self):\n",
    "        #  This Function will fill backqueues until all the backqueues are filled or All or FQueues are empited\n",
    "        for q in self.BQueues:\n",
    "            while (q.empty):\n",
    "                U = None\n",
    "                while True:\n",
    "                    U = self.FQueues[self.prioritizer()].get_URL()\n",
    "                    if U is not None:\n",
    "                        break\n",
    "\n",
    "                if U is not None:\n",
    "                    # Check iF U already has a queue allocated to it\n",
    "                    r, p = self.__has_domain__(U)\n",
    "                    if r is True:\n",
    "                        p.add_URL(U)\n",
    "                    else:\n",
    "                        q.add_URL(U)\n",
    "\n",
    "    def get_URL(self,threadID):  # This function will be called by a thread to get a URL from the Back Queue\n",
    "        ret = self.TimeHeap.get()\n",
    "        if (time() - ret[0]) < 0:\n",
    "            # we have to wait\n",
    "            print(\"Thread \" + str(threadID) + \" waiting for \" + str(round(ret[0]-time())) + \" seconds\")\n",
    "            sleep(round(ret[0] - time()))\n",
    "\n",
    "        url = self.BQueues[ret[1]].get_URL()  # get URL from Bqueue\n",
    "\n",
    "        if self.BQueues[ret[1]].empty:\n",
    "            self.fill_Backqueue()  # if the queue is empty fill up the queue\n",
    "\n",
    "        self.TimeHeap.put((ret[0] + WAITTIME, ret[1]))  # Update the heap\n",
    "\n",
    "        return url\n",
    "\n",
    "  \n",
    "    def prioritizer(self, f=None, URL=None):\n",
    "        \"\"\"\n",
    "        Take URL and returns priority from 1 to F\n",
    "        Right now it like a stub function.\n",
    "        It will return a random number from 1 to f for given inputs.\n",
    "        \"\"\"\n",
    "        return random.randint(0, len(self.FQueues) - 1)\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def thread_func(threadID, Frontier):  # The thread Function will take a frontier Object and a threadID as  arguments\n",
    "    global urls_fetched\n",
    "    k = 0\n",
    "    while urls_fetched < 10:\n",
    "        u = Frontier.get_URL(threadID)\n",
    "        print(\"Thread \" + str(threadID) + \": fetching url: \" + u)\n",
    "\n",
    "        URLList, content = Frontier.fetcher.fetch_Content(u)\n",
    "        print(\"Thread \" + str(threadID) + \": \" + str(len(URLList)) + \" Raw Urls fetched\")\n",
    "        print(\"Thread \" + str(threadID) + \": Dumping Raw Content to Disk\")\n",
    "        Frontier.storage.PutContent(u, content)\n",
    "        print(\"Thread \" + str(threadID) + \": Testing Urls\")\n",
    "        # Removing Duplicate, those which are not allowed to be parsed and those mime type is not text/html\n",
    "        i = Frontier.add_urls_list(URLList)\n",
    "        print(\"Thread \" + str(threadID) + \": Added \" + str(i) + \" Urls to Fqueues after parsing \")\n",
    "        urls_fetched += 1\n",
    "        k += 1\n",
    "\n",
    "    print(\"Thread \" + str(threadID) + \": Total URLS fetched: \" + str(k))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initlize Variables and Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = frontier(FRONTQUEUES, BACKQUEUES, Filename)\n",
    "f.add_URL(\"https://docs.oracle.com/en/\")\n",
    "f.add_URL(\"https://www.oracle.com/corporate/\")\n",
    "f.add_URL(\"https://en.wikipedia.org/wiki/Machine_learning\")\n",
    "f.add_URL(\"https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\")\n",
    "f.add_URL(\"https://docs.oracle.com/middleware/jet210/jet/index.html\")\n",
    "f.add_URL(\"https://en.wikipedia.org/w/api.php\")\n",
    "f.add_URL(\"https://en.wikipedia.org/api/\")\n",
    "f.add_URL(\"https://en.wikipedia.org/wiki/Weka_(machine_learning)\")\n",
    "f.fill_Backqueue()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thread 0: fetching url: https://en.wikipedia.org/wiki/Machine_learning\n",
      "Thread 1: fetching url: https://docs.oracle.com/en/Thread 2: fetching url: https://www.oracle.com/corporate/\n",
      "\n",
      "Thread 3 waiting for 12 seconds\n",
      "Thread 4 waiting for 12 seconds\n",
      "Thread 5 waiting for 12 seconds\n",
      "Thread 2: 41 Raw Urls fetched\n",
      "Thread 2: Dumping Raw Content to Disk\n",
      "Thread 2: Testing Urls\n",
      "Thread 1: 0 Raw Urls fetched\n",
      "Thread 1: Dumping Raw Content to Disk\n",
      "Thread 1: Testing Urls\n",
      "Thread 1: Added 0 Urls to Fqueues after parsing \n",
      "Thread 0: 1347 Raw Urls fetched\n",
      "Thread 0: Dumping Raw Content to Disk\n",
      "Thread 0: Testing Urls\n",
      "Url Added to Front Queues: https://www.oracle.com/corporate/accessibility/\n",
      "Thread 3: fetching url: https://en.wikipedia.org/w/api.php\n",
      "Thread 5: fetching url: https://www.csie.ntu.edu.tw/~cjlin/libsvm/index.html\n",
      "Thread 6 waiting for 15 seconds\n",
      "Thread 7 waiting for 15 seconds\n",
      "url: https://www.oracle.com/ Rejected, Invalid mime type (Not text/html)\n",
      "Url Added to Front Queues: https://en.wikipedia.org/wiki/Machine_Learning_(journal)\n",
      "Url Added to Front Queues: https://www.oracle.com/universal-menu/\n",
      "Url Added to Front Queues: https://www.oracle.com/cloud/sign-in.html\n",
      "Thread 3: 304 Raw Urls fetched\n",
      "Thread 3: Dumping Raw Content to Disk\n",
      "Thread 3: Testing Urls\n",
      "Url Added to Front Queues: https://en.wikipedia.org/wiki/Statistical_learning_in_language_acquisition\n",
      "Url Added to Front Queues: https://www.oracle.com/cloud/free/?source=:ow:o:h:nav:OHP0625ViewAccountsButton&intcmp=:ow:o:h:nav:OHP0625ViewAccountsButton\n",
      "Thread 5: 58 Raw Urls fetched\n",
      "Thread 5: Dumping Raw Content to Disk\n",
      "Thread 5: Testing Urls\n",
      "Url Added to Front Queues: https://en.wikipedia.org/wiki/Data_mining\n",
      "url: https://en.wikipedia.org/wiki/File:Multi-Layer_Neural_Network-Vector-Blank.svg Rejected, Invalid mime type (Not text/html)\n",
      "url: http://www.python.org/download/ Rejected, Invalid mime type (Not text/html)\n",
      "Url Added to Front Queues: https://www.oracle.com/webapps/redirect/signon?nexturl=\n",
      "Url Added to Front Queues: https://en.wikipedia.org/wiki/Statistical_classification\n"
     ]
    }
   ],
   "source": [
    "threads = []\n",
    "for i in range(THREADS):\n",
    "    t = threading.Thread(target=thread_func, args=[i, f])\n",
    "    t.start()\n",
    "    threads.append(t)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Had to Terminate the Crawler myself as the supply of urls is limitless :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ------------------------------------------------------End of Notebook---------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
